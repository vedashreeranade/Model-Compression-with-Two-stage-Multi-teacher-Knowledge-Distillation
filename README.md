## Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System
- A Two-stage Multi-teacher Knowledge Distillation method for web Question Answering system was proposed in the paper.
- A general Q&A distillation task for student model pre-training was developed, and further this pretrained student model  was fine-tune with multi-teacher knowledge distillation on downstream tasks (like Web Q&A task, MNLI, SNLI, RTE tasks
from GLUE), which effectively reduces the overfitting bias in individual teacher models, and transfers more general knowledge to the student model. 
- The experiment results show that the method can significantly outperform the baseline methods and even achieve comparable results with the original teacher models, along with substantial speedup of model inference.

